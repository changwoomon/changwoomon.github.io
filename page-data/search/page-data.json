{"componentChunkName":"component---src-pages-search-jsx","path":"/search/","result":{"data":{"allMarkdownRemark":{"nodes":[{"excerpt":"Transformer-DST 1. Key Idea 이전 논문들에서는 Encoder에서 BERT를 사용하지만, Value Generation부분에서는 RNN Decoder를 사용하는 아이러니.. Purely Transformer-based…","fields":{"slug":"/Transformer-DST/"},"frontmatter":{"date":"May 12, 2021","title":"Jointly Optimizing State Operation Prediction and Value Generation for Dialogue State Tracking","tags":["Paper Review","DST","Dialogue State Tracking","Transformer-DST"]},"rawMarkdownBody":"\r\n## Transformer-DST\r\n\r\n## 1. Key Idea\r\n\r\n- 이전 논문들에서는 Encoder에서 BERT를 사용하지만, Value Generation부분에서는 RNN Decoder를 사용하는 아이러니..\r\n- Purely Transformer-based framework를 사용\r\n즉, Single BERT가 **Encoder와 Decoder 모두**에서 work\r\n→ 이렇게 하면 prediction objective와 value generation objective가 BERT 하나만 optimize하게 됨\r\n- Encoder(BERT)에서 사용한 hidden states 값을 Decoder(BERT)에서 Re-use가 가능해짐\r\n\r\n## 2. Input\r\n\r\n- Input으로는 SOM-DST와 같은 구조를 지님\r\n- 아래 그림과 같이 **D1, D2, S1을 INPUT**으로 넣고, **OUTPUT으로 S2**를 출력\r\n\r\n<img src=\"input.PNG\" height=150>\r\n\r\n- $$D_{t-1}, D_{t}, S_{t-1}$$을 INPUT으로 $$S_{t}$$를 PREDICT\r\n- $$D_{t}$$ : t번째 dialogue turn의 (System utterance, User response) pair\r\n- $$S_{t}$$ : $$(d_{j}, s_{j}, v_{j}) | 1<=j<=J$$\r\n\r\n여기서 $$d$$는 domain, $$s$$는 slot, $$v$$는 value를 나타냄\r\n\r\n만약, 아무 정보도 없을시 $$(d_{j}, s_{j})$$로 나타내고, $$v_{j}$$는 NULL\r\n\r\n## 3. Overview\r\n\r\n- 왼쪽은 Transformer Encoder, 오른쪽은 Transformer Decoder\r\n- Encoder (왼쪽)에서 $$h_{sl}^{L}$$ (hidden state)를 뽑아내고, Decoder (오른쪽)에서 Re-use하는 방법으로 사용됨\r\n- Decoder (오른쪽)은 left-to-right attention (왼쪽에서 오른쪽으로 차례로 출력하는 language model, 즉 왼쪽 출력값이 오른쪽 입력)\r\n\r\n<img src=\"overview.PNG\">\r\n\r\n## 4. Encoder\r\n\r\n<img src=\"encoder.PNG\">\r\n\r\n- Encoder의 Input은 $$D_{t-1}, D_{t}, S_{t-1}$$ 3가지가 들어감\r\n    - $$D_{t}$$는 t번째 turn의 ( System Utterance, User Resposne ) pair\r\n    - $$S_{t-1}$$은 $$(d_{j}, s_{j}, v_{j}) | 1<= j <= J$$\r\n\r\n### 4-1) Encoder Input\r\n\r\n<img src=\"encoder_input.PNG\">\r\n\r\n- $$[SLOT] \\bigoplus d_{j} \\bigoplus - \\bigoplus s_{j} \\bigoplus - \\bigoplus v_{j}$$ 으로 구성\r\n    - $$\\bigoplus$$는 concat을 나타냄\r\n    - 총 $$J$$개의 domain-slot에 대해서 만들어줌\r\n    - $$[SLOT] \\bigoplus d_{j} \\bigoplus - \\bigoplus s_{j} \\bigoplus - \\bigoplus v_{j}$$  * J번\r\n- $$[SLOT]$$은 Transformer block을 통과한 후, $$X^{l}_{sl_{j}}$$ 형태로 출력되며, Prediction (CARRYOVER, ..., UPDATE 등)으로 사용됨\r\n\r\n### 4-2) Multi-head self-attention\r\n\r\n<img src=\"multi-head.png\">\r\n\r\n- Multi-head Self-attention 매카니즘 사용\r\n\r\n<img src=\"multi-head2.png\">\r\n\r\n- 여기서 $$M^{x}$$ : self-attention mask matrix\r\n    - $$M^{x} \\in R^{|x| \\times |x|}$$\r\n    - $$M^{x}_{ij} \\in \\{0, - \\infty \\}$$\r\n    - $$M^{x}_{ij} = 0$$이면 i-th position이 j-th position에 attend하다는 의미\r\n    - $$M^{x}_{ij} = -\\infty$$이면 i-th position과 j-th position을 prevents하겠다는 의미\r\n\r\n### 4-3) Encoder Output\r\n\r\n$$X^{L} = [x^{L}_{cls}, x^{L}_{1}, ..., x^L_{sl_{1}}, ..., x^{L}_{sl_{J}},...]$$\r\n\r\n### 4-4) Encoder Objective\r\n\r\n- Encoder outputs $$x^{L}_{sl_{j}}$$에서 $$[SLOT]$$칸에 해당하는 값을 확인\r\n- CARRYOVER, DELETE, DONTCARE, UPDATE\r\n- UPDATE의 경우에만 decoder generater에서 사용함\r\n\r\n## 5. Decoder (Slot Value Generation)\r\n\r\n<img src=\"decoder.PNG\">\r\n\r\n- Left-to-right self-attention을 사용함\r\n- Encoder에서 도출해낸 hidden states를 decoder에서 reuse\r\n- Resue의 의미는 hidden state를 decoder에서 다시 한번 계산할 필요가 없어진다는 의미를 갖음.\r\n\r\n### 5-1) Decoder Input\r\n\r\n<img src=\"decoder_input.PNG\">\r\n\r\n- Encoder (reuse)\r\n    - 왼쪽의 $$D_{t}$$와 $$[SLOT]$$은 Encoder 부분을 나타냄\r\n    - 현재 turn의 $$D_{t}$$의 hidden state vector를 사용함\r\n    - $$[SLOT]$$ 중 UPDATE로 prediction이 된 hidden state vector만 사용함\r\n- Decoder\r\n    - $$[BOS]$$는 String의 시작\r\n    - $$w^{v_{j}}_1, w^{v_{j}}_2$$는 decoder의 output을 다시 input으로 가지고 와서 사용 (left-to-right self-attention)\r\n\r\n### 5-2) Left-to-right self-attention\r\n\r\n<img src=\"left-to-right.png\">\r\n\r\n<img src=\"left-to-right2.png\">\r\n\r\n- 일반 Multi-head attention하고 비슷\r\n- $$\\hat{X}$$ : re-used된 encoder hidden states\r\n- $$Y$$ : Decoder hidden states\r\n- $$\\hat{X}$$와 $$Y$$를 concat해서 사용\r\n- 만약 $$j \\leq i$$일때,  $$M^{y}_{ij} = 0$$ 으로 사용 (left-to-right attention)\r\n\r\n### 5-3) Decoder Objective\r\n\r\n- **Generated slot value loss**와 **ground-truth slot value**를 비교해서  Loss를 산출\r\n- Teacher Forcing을 모든 time에서 사용\r\n\r\n# 결과\r\n\r\n- MutliWOZ 2.0과 MultiWOZ 2.1에서 제출 당시 SOTA\r\n\r\n<img src=\"result.png\">\r\n\r\n- 각각의 Domain의 Joint goal Accuracy를 비교\r\n    - 신기한 점은 Taxi 빼고 (다른 모델보다) 높은 성능을 보임\r\n    - 이유를 찾아보니 Taxi의 경우 Train과의 co-occurrence relations가 있음\r\n    - 하지만 Ours에는 이러한 점을 해결하려고 하지는 않았음\r\n\r\n<img src=\"result2.png\">\r\n\r\n- 시간의 경우 SOM-DST보다는 Inference time이 오래걸림\r\n\r\n<img src=\"result3.png\">\r\n\r\n- Reuse를 사용했을 경우에도 여러가지 방법을 시도해보았고, $$D_{t} + [SLOT]$$을 사용했을 때 Joint Accuracy에서 좋은 성능을 보였음\r\n\r\n<img src=\"result4.png\">\r\n\r\n---\r\n\r\n## 참고 자료\r\n\r\n- 논문: [Jointly Optimizing State Operation Prediction and Value Generation for Dialogue State Tracking](https://arxiv.org/abs/2010.14061)\r\n- GitHub: [Transformer-DST](https://github.com/zengyan-97/Transformer-DST)\r\n"},{"excerpt":"SOM-DST SOM-DST 기존 모델의 문제점 Ontology-based DST 실제 시나리오에 잘 대응하지 못함 unseen value를 처리할 수 없음 ontology가 많으면 처리 시간이 오래 걸림 Open-vocab-based DST…","fields":{"slug":"/SOM-DST/"},"frontmatter":{"date":"May 10, 2021","title":"Efficient Dialogue State Tracking by Selectively Overwriting Memory","tags":["Paper Review","DST","Dialogue State Tracking","SOM-DST"]},"rawMarkdownBody":"\r\n## SOM-DST\r\n\r\n![SOM-DST](https://github.com/clovaai/som-dst/raw/master/img/overview6.png)\r\n\r\n## 기존 모델의 문제점\r\n\r\n### Ontology-based DST\r\n\r\n- 실제 시나리오에 잘 대응하지 못함\r\n- unseen value를 처리할 수 없음\r\n- ontology가 많으면 처리 시간이 오래 걸림\r\n\r\n### Open-vocab-based DST (TRADE)\r\n\r\n- turn마다 slot의 모든 value를 생성해야해서 비효율적임\r\n\r\n---\r\n\r\n## Definition\r\n\r\n$$t$$: turn\r\n\r\n$$S^j$$: slot\r\n\r\n$$V^j_t$$: corresponding slot value\r\n\r\n$$J$$: total number of such slots\r\n\r\n$$r^j_t\\in O = \\left\\{\\rm\\textbf{CARRYOVER, DELETE, DONTCARE, UPDATE}\\right\\}$$\r\n\r\n$$A_t$$: System response\r\n\r\n$$U_t$$: User utterance\r\n\r\n---\r\n\r\n## State Operation Predictor (Encoder)\r\n\r\nEncoder 모델로 **pretrained BERT encoder** 사용\r\n\r\n### <span style=\"background-color:#fcffb0\">Encoder Input을 만들기 위한 준비물</span>\r\n\r\n$$D_t = A_t ⊕ ; ⊕ U_t ⊕ [SEP]$$: dialogue utterances at turn t\r\n\r\n- `;` $A_t$와 $U_t$를 구분하기 위한 스페셜 토큰\r\n- `[SEP]` dialogue turn이 끝났다는 것을 표시하기 위한 스페셜 토큰\r\n\r\n$$B_t^j = [SLOT]^j ⊕ S^j ⊕ - ⊕ V_t^j$$ : representation of the j-th slot-value pair\r\n\r\n- j-th slot-value pair를 하나의 벡터로 aggregate\r\n- $$`[SLOT]^j`$$\r\n\r\n    [SLOT] 이라는 스페셜 토큰을 사용\r\n\r\n    BERT의 [CLS] 토큰과 같은 역할\r\n\r\n- $$`V_t^j`$$\r\n\r\n    <img src=\"V_t^j.PNG\" height=150>\r\n\r\n$$B_t = B_t^1 ⊕ ... ⊕ B_t^J$$ : representation of the dialogue state at turn t\r\n\r\n### Encoder Input\r\n```\r\n$$X_t = [CLS] ⊕ D_{t-1} ⊕D_t ⊕ B_{t-1}$$\r\n\r\nsegment id:        0       1         1\r\n```\r\n⇒ **Input : Sum($X_t$ embedding, segment id embedding, positional embedding)**\r\n\r\ndialogue history로 이전 턴의 dialogue utterances $$D_{t-1}$$을 사용한다.\r\ndialogue history의 size: 1\r\n\r\n모델이 입력으로 들어오는 dialogue 간의 `Markov property`를 가정\r\n\r\n이전 turn dialogue state $$B_{t-1}$$은 전체 dialogue history를 압축적으로 표현하는 역할\r\n\r\n### Encoder Output\r\n\r\n$$H_t \\in \\mathbb R^{\\left|X_t\\right|\\times d}$$ : $$h_t^{\\rm X}$$ $$(t=1...t)$$ 까지 집합\r\n\r\n$$h_t^{[CLS]}, h_t^{[SLOT]^j} \\in \\mathrm R^d$$\r\n\r\n- $$[CLS], [SLOT]^j$$에 대응하는 output\r\n\r\n$$h_t^X = tanh(W_{pool} h_t^{[CLS]})$$\r\n\r\n- $$h_t^X$$: 전체 input $$X_t$$를 sequence로 aggregate\r\n- $$W_{pool} \\in \\mathbb R^{d\\times d}$$: feed-forward layer with a learnable parameter\r\n\r\n### State Operation Prediction\r\n\r\n$$P^j_{opr, t} = softmax(W_{opr} h_t^{[SLOT]^j})$$\r\n\r\n- $$W_{opr} \\in \\mathbb R^{\\left| O\\right|\\times d}$$ : learnable parameter\r\n- $$P_{opr, t}^j \\in \\mathbb R^{\\left| O\\right|}$$ : j-th slot의 turn t에서의 연산에 대한 확률 분포\r\n- SOM-DST에서는 $$\\left| O\\right| = 4$$,\r\n\r\n    $$O = \\left\\{\\rm{CARRYOVER, DELETE, DONTCARE, UPDATE}\\right\\}$$\r\n\r\n→ $$r_t^j = argmax(P_{opr, t}^j)$$\r\n\r\n→ slot의 Operation의 결과가 `UPDATE` 일 때 slot value를 generation\r\n\r\n- Encoder에서 나온 Operation의 결과가 `Update`인 경우를 집합으로 표현하면\r\n\r\n    $$\\mathbb{U}_t = \\left\\{j|r_t^j = \\rm{UPDATE}\\right\\}$$, and its size as $$J_t^\\prime = \\left| \\mathbb{U}_t\\right|$$\r\n\r\n    Recab for V\r\n\r\n    <img src=\"V_t^j.PNG\" height=150>\r\n\r\n---\r\n\r\n## Slot Value Generator (Decoder)\r\n\r\n- Encoder에서 나온 Operation의 결과가 `Update` 인 경우 해당 slot의 value를 예측\r\n- SOM-DST의 generator는 value를 $$J$$개가 아닌 $$J^\\prime_t$$개의 slot에 대해서만 만들어준다.\r\n\r\n    대부분의 경우에서 $$J^\\prime_t \\ll J$$이기 때문에 더 효율적이라고 주장\r\n\r\n- Decoder 모델로 **GRU** 사용\r\n    - 입력으로 word embedding vector $$e_t^{j,k}$$를 받으면서 **GRU**의 hidden state vector $$g_t^{j, k}$$를 recurrent하게 업데이트\r\n    - $$g_t^{j, 0} = h_t^{\\rm x}$$, $$e_t^{j,0} = h_t^{[slot]j}$$: **GRU**에 들어가는 초기값\r\n    - $$g_t^{j, k} = GRU(g_t^{j, k-1}, e_t^{j,k})$$\r\n    - $$e_t^{j,k}$$가 [EOS] 토큰이 나올때까지 진행\r\n    - hidden state $$g_t^{j, k}$$는 k-th decoding step을 거치면서 vocabulary 와 user utterance의 단어에 대한 확률 분포로 변함\r\n\r\n        $$P^{j, k}_{vcb, t} = softmax(Eg^{j, k}_t) \\in \\mathbb R^{d_{vcb}}$$\r\n\r\n        - $$E \\in \\mathbb R^{d_{vcb}\\times d}$$: Encoder와 Decoder가 서로 공유하는 word embedding matrix\r\n            - $$d_{vcb}$$: vocabulary size\r\n\r\n        $$P^{j, k}_{ctx, t} = softmax(H_t g_t^{j, k}) \\in \\mathbb R^{\\left|X_t\\right|}$$\r\n\r\n        - user utterance의 단어에 대한 확률 분포\r\n\r\n        $$P^{j, k}_{val, t} = \\alpha P^{j, k}_{vcb, t} + (1-\\alpha) P^{j, k}_{ctx, t}$$: final output distribution\r\n\r\n        - $$\\alpha = sigmoid(W_1 \\left[g^{j, k}_t ; e^{j, k}_t ; c^{j, k}_t\\right])$$\r\n            - $$W_1 \\in \\mathbb R^{1\\times (3d)}$$: learnable parameter\r\n            - $$c^{j, k}_t = P^{j, k}_{ctx, t} H_t \\in \\mathbb R^d$$: context vector\r\n\r\n---\r\n\r\n## Objective Function\r\n\r\n### State operation predictor\r\n\r\n**Main Task**\r\n\r\nstate operation classification\r\n\r\n**Auxiliary Task**\r\n\r\ndomain classification\r\n\r\nstate operation classification 외에도 domain classification을 보조 task로 사용하여 모델이 dialogue turn 간의 slot operation과 domain transition의 상관 관계를 학습하도록 함\r\n\r\n$$P_{dom, t} = softmax(W_{dom} h_t^{\\rm X})$$\r\n\r\n- $$W_{dom} \\in \\mathbb R^{d_{dom}\\times d}$$: learnable parameter\r\n- $$P_{dom, t} \\in \\mathbb R^{d_{dom}}$$: turn t에서 domain에 대한 확률 분포\r\n    - $$d_{dom}$$: # of domains defined in the dataset\r\n\r\n**Average of the negative log-likelihood**\r\n\r\n$$L_{opr, t} = -\\frac{1}{J}\\sum_{j=1}^{J}(Y_{opr, t}^j)^\\top log(P^j_{opr, t})$$\r\n\r\n$$L_{dom, t} = -(Y_{dom, t})^\\top log(P_{dom, t})$$\r\n\r\n- $$Y_{dom, t} \\in \\mathbb R^{d_{dom}}$$: one-hot vector for the ground truth domain\r\n- $$Y^j_{opr, t} \\in \\mathbb R^{\\left| O\\right|}$$: one-hot vector for the ground truth operation for the j-th slot\r\n\r\n### Slot value generator\r\n\r\n**Average of the negative log-likelihood**\r\n\r\n$$L_{svg, t} = -\\frac{1}{\\left|\\mathbb U_t\\right|}\\sum_{j\\in\\mathbb U_t}^{}\\left[\\frac{1}{K^j_t}\\sum_{k=1}^{K^j_t}(Y_{val, t}^{j, k})^{\\top}log(P^{j, k}_{val, t})\\right]$$\r\n\r\n- $$K_t^j$$: # of tokens of the ground truth value that needs to be generated for the j-th slot\r\n- $$Y_{val, t}^{j, k} \\in \\mathbb R^{d_{vcb}}$$: one-hot vector for the ground truth token that needs to be generated for the j-th slot at the k-th decoding step\r\n\r\n### Final Loss\r\n\r\nto minimized $$L_{joint, t} = L_{opr, t} + L_{dom, t} + L_{svg, t}$$\r\n\r\n---\r\n\r\n## Experimental Setup\r\n\r\n### Datasets\r\n\r\nMultiWOZ 2.0 and MultiWOZ 2.1\r\n\r\n### Training\r\n\r\n- Encoder : Bert-base-uncased\r\n- Decoder : GRU\r\n- Hidden size : 768\r\n- Optimizer : BertAdam\r\n- Encoder LR and warmup : 4e-5, 0.1\r\n- Decoder LR and warmup : 1e-4, 0.1\r\n- Batch size : 32\r\n- Dropout : 0.1\r\n- Word Dropout 적용, 0.1확률로 word 를 [UNK] 로 바꿈\r\n- Input max length : 256\r\n- Training Epoch : 30\r\n\r\n---\r\n\r\n## 결과\r\n\r\n### Joint Goal Accuracy\r\n\r\n<img src=\"joint_goal_accuracy.PNG\" height=400>\r\n\r\n> † indicates the case where BERT-large is used for our model\r\n\r\n### Domain-specific Accuracy\r\n\r\n<img src=\"domain-specific_accuracy.PNG\" height=450>\r\n\r\n### Latency\r\n\r\n<img src=\"latency.PNG\" height=150>\r\n\r\n---\r\n\r\n## 평가\r\n\r\n- JGA, Domain-specific Accuracy 에서 SOTA 혹은 비슷한 수준의 성능을 보여줌\r\n- inferecnce 타임이 매우 짧음에도 불구하고 좋은 성능을 보여줌\r\n\r\n---\r\n\r\n## 참고 자료\r\n\r\n- 논문: [Efficient Dialogue State Tracking by Selectively Overwriting Memory](https://arxiv.org/abs/1911.03906)\r\n- 영상: [[Paper Review] SOM-DST : Efficient Dialogue State Tracking by Selectively Overwriting Memory - KoreaUniv DSBA](https://www.youtube.com/watch?v=7Nwe2BBUZsw)\r\n- GitHub: [SOM-DST](https://github.com/clovaai/som-dst)\r\n"},{"excerpt":"CHAN-DST slot imbalance 문제를 해결하고자 adaptive objective를 도입 a contextual hierarchical attention network (CHAN)를 사용: dislogue history에서 relevant…","fields":{"slug":"/CHAN-DST/"},"frontmatter":{"date":"May 07, 2021","title":"A Contextual Hierarchical Attention Network with Adaptive Objective for Dialogue State Tracking","tags":["Paper Review","DST","Dialogue State Tracking","CHAN-DST"]},"rawMarkdownBody":"\r\n## CHAN-DST\r\n\r\n- slot imbalance 문제를 해결하고자 adaptive objective를 도입\r\n- a contextual hierarchical attention network (CHAN)를 사용: dislogue history에서 relevant context를 찾기 위함  \r\n  → 각 턴의 발화로부터 word-level 관련 정보 검색  \r\n  → contextual representation으로 encode  \r\n  → 모든 context표현을 turn-level관련 정보로 집계한 후 word-level 정보와 합친 output 생성  \r\n- state transition prediction task\r\n\r\n---\r\n\r\n## Definition\r\n\r\n- $$T$$: turn\r\n- $$U_t$$: user utterance of turn t\r\n- $$R_t$$: system response of turn t\r\n- $$X$$: $$\\left\\{(U_1, R_1), ... , (U_T, R_T)\\right\\}$$\r\n- $$B_t$$: $$\\left\\{(s, v_t), s \\in S\\right\\}$$\r\n- $$S$$: set of slots\r\n- $$v_t$$: corresponding value of the slot $$s$$\r\n- slot: concatenation of a domain name and a slot name\r\n\r\n---\r\n\r\n## Contextual Hierarchical Attention Network\r\n\r\n<img src=\"structure.png\">\r\n\r\n### 1. Sentence Encoder\r\n\r\n<img src=\"sentence_encoder.png\">\r\n\r\n`utterance encoder`\r\n\r\n- BERT special token 사용  \r\n  → [CLS] : 문장의 representation들을 합치기위해 사용 (to aggregate the whole representation of a sentence)  \r\n  → [SEP] : 문장의 끝을 나타내기위해 사용.  \r\n- $$U_t = \\left\\{w_1^u, ..., w_l^u\\right\\}$$ (user utterance)  \r\n  $$R_t = \\left\\{w_1^r, ..., w_{l'}^r\\right\\}$$ (system response)  \r\n  $$t$$: dialogue turn  \r\n- $$h_t = BERT_{finetune}([R_t;U_t])$$  \r\n  ($$h_t$$: contextual word representations)  \r\n- 여기서 BERT finetune은 training도중 finetuning이 될것을 의미.\r\n\r\n`slot-value encoder`\r\n\r\n- $$BERT_{fixed}$$는 contextual semantics vectors로 encode해준다.\r\n- utterance encode할때와 다른 점은 [CLS] 토큰의 output vector를 전체 문장 representation할때 사용한다. (to obtain the whole sentence representation)\r\n- $$h_s = BERT_{fixed}(s)$$  \r\n  $$h_t^v = BERT_{fixed}(v_t)$$  \r\n- $$BERT_{fixed}$$는 training 도중 고정되어있다. 그래서 우리 모델은 unseen slots and values에 대해서 original BERT representation로 확장해서 보는게 가능하다.\r\n\r\n### 2. Slot-Word Attention\r\n\r\n<img src=\"slot-word_attention.png\">\r\n\r\n- slot-word attention은 multi-head attention을 사용한다.\r\n- $$c_{s,t}^{word} = MultiHead(h^s, h_t, h_t)$$\r\n\r\n### 3. Context Encoder\r\n\r\n<img src=\"context_encoder.png\">\r\n\r\n- context encoder : unidirectional transformer encoder\r\n- {1, ..., t} 턴에서 추출 된 word-level slot-related 정보의 contextual relevance를 모델링하기 위한 것.\r\n- $$N$$개의 idenctical한 layer가 있다.\r\n  - 각 layer는 2개의 sub-layer를 가지고 있다.\r\n  - 첫번째 sub-layer: masked multi-head self-attention(Q = K = V)\r\n  - 두번째 sub-layer: position-wise fully connected feed-forward network(FFN) (two linear transformations, RELU activation으로 구성)\r\n  - $$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$$\r\n- $$m^n = FFN(MultiHead(m^{n-1}, m^{n-1}, m^{n-1}))$$  \r\n  $$m^0 = [c_{s,1}^{word} + PE(1), ..., c_{s,t}^{word} + PE(t)]$$  \r\n  $$c_{s,\\leq t}^{ctx} = m^N$$  \r\n- $$m^n$$: n번째 context encoder레이어의 아웃풋\r\n  $$PE(.)$$: positional encoding function\r\n\r\n### 4. Slot-Turn Attention\r\n\r\n<img src=\"slot-turn_attention.png\">\r\n\r\n- turn-level relevant information을 contextual representation에서 검출해내기 위해 사용\r\n- $$c_{s,t}^{turn} = MultiHead(h^s, c_{s,\\leq t}^{ctx},c_{s,\\leq t}^{ctx})$$\r\n- 이로인해 word-level and turn-level 의 relevant information을 historical dialogues에서 얻어낼 수 있다.\r\n\r\n### 5. Global-Local Fusion Gate\r\n\r\n<img src=\"global-local-fusion_gate.png\">\r\n\r\n- global context와 local utterance의 균형을 맞추기 위해, contextual information과 current turn information의 비율을 조절함.\r\n- $$c_{s,t}^{word},~~ c_{s,t}^{turn}$$에 따라 global과 local정보가 어떻게 결합되어야할지 알려주는 fusion gate mechanism을 사용\r\n- $$g_{s,t} = \\sigma(W_g\\bigodot[c_{s,t}^{word};c_{s,t}^{turn}])$$\r\n- $$c_{s,t}^gate = g_{s,t}\\bigotimes c_{s,t}^{word} + (1-g_{s,t}\\bigotimes c_{s,t}^{turn})$$\r\n  - $$W_g \\in R^{2d\\times d}$$\r\n  - $$\\sigma$$: Sigmoid\r\n  - $$\\bigodot$$, $$\\bigotimes$$\r\n- $$o_{s,t}$$ = LayerNorm(Linear(Dropout($c_{s,t}^{gate}$)))\r\n- **value $$v_t$$에 대한 확률분포와 training objective**  \r\n  $$p(v_t|U_{\\leq t},~R_{\\leq t}, s) = exp(-||o_{s,t} - h_t^v||2)\\over {\\sum{v'\\in V_s}exp(-||o_{s,t}-h_t^{v'}||2)}$$  \r\n  *$$L{dst}$$* $$= \\sum_{s\\in S}\\sum ^T_{t = 1}-log(p(\\hat v_t|U_{\\leq t},~R_{\\leq t}, s))$$  \r\n  - $$V_s$$: candidate value set of slot s\r\n  - $$\\hat v_t \\in V_s$$: ground-truth value of slot s\r\n\r\n---\r\n\r\n## State Transition Prediction\r\n\r\n<img src=\"state_transition_prediction.png\">\r\n\r\n- relevant context를 더 잘 포착하기 위해, auxiliary binary classification task사용.\r\n- $$c_{s,t}^{stp} = tanh(W_c \\odot c_{s,t}^{gate})$$\r\n- $$p_{s,t}^{stp} = \\sigma (W_p \\odot [c_{s,t}^{stp};c_{s, t-1}^{stp}])$$\r\n  - $$W_c \\in \\R^{d\\times d}$$\r\n  - $$W_c \\in \\R^{2d}$$\r\n  - $$t = 1$$일때는 $$c_{s,t}^{stp}$$와 zero vectors를 concat함.\r\n- binary CE loss ($$y_{s,t}^{stp}$$: ground-truth transition labels // $$p_{s,t}^{stp}$$: transition probability)\r\n- $$L_{stp} = \\sum_{s\\in S}\\sum_{t = 1}^T -y_{s,t}^{stp}~.~log(p_{s,t}^{stp})$$\r\n\r\n---\r\n\r\n## Adaptive Objective\r\n\r\n- hard slots와 samples에 관한 optimization을 encourage한다.\r\n- all slots의 learning을 balancing함.\r\n- $$acc_s^{val}$$: accuracy of slot s on validation set\r\n- `slot-level difficulty`\r\nif $$acc_s^{val} \\leq acc_{s'}^{val}$$;  \r\n  → slot s 가 slot s'보다 더 어려운 것.  \r\n  → $$\\alpha$$: slot-level difficulty  \r\n  - $$\\alpha_s = {1 - acc_s^{val} \\over {\\sum_{s'\\in S} 1-acc_{s'}^{val}}} \\cdot |S|$$\r\n- `sample-level difficulty`  \r\n  → Suppose there are two samples $$\\left\\{(U_t, R_t),(s, v_t)\\right\\}$$ and $$\\left\\{(U_{t'}, R_{t'}),(s', v_{t'})\\right\\}$$  \r\n  → 만약 former confidence 가 latter보다 더 낮다면, 첫번째 sample이 두번째보다 더 어려운 것.  \r\n  → $$\\beta$$: sample level difficulty  \r\n  - $$\\beta(s, v_t) = (1 - p(s, v_t))^\\gamma$$\r\n  - $$p(s,v_t)$$: confidence of sample $$(U_t, R_t),(s, v_t)$$  \r\n  $$\\gamma$$: hyper-parameter\r\n- $$L_{adapt}(s,v_t) = -\\alpha_s\\beta(s, v_t)log(p(s, v_t))$$\r\n- slot s가 평균 slot의 difficulty보다 높다면, $$\\alpha_s$$는 s에 대한 loss를 키울 것이다. 비슷하게, sample의 optimization이 low confidence를 갖고 있다면 loss는 커질것이다.\r\n\r\n---\r\n\r\n### Optimization\r\n\r\n- During joint training, we optimize the sum of these two loss functions as following  \r\n  $$L_{joint} = L_{dst} + L_{stp}$$  \r\n- At the fine-tuning phase, we adopt the adaptive objective to fine-tune DST task as following  \r\n  $$L_{finetune} = \\sum_{s\\in S}\\sum^T_{t=1}L_{adapt(s, \\hat v_t)}$$  \r\n\r\n---\r\n\r\n## 참고 자료\r\n\r\n- 논문: [A Contextual Hierarchical Attention Network with Adaptive Objective for Dialogue State Tracking](https://www.aclweb.org/anthology/2020.acl-main.563.pdf)\r\n- GitHub: [CHAN-DST](https://github.com/smartyfh/CHAN-DST)\r\n"}]}},"pageContext":{}},"staticQueryHashes":[]}