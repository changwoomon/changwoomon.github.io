{"componentChunkName":"component---src-pages-search-jsx","path":"/search/","result":{"data":{"allMarkdownRemark":{"nodes":[{"excerpt":"pooling-based deep recurrent neural network (PDRNN) DRNN architecture The RNN maps the input vector  to the equivalent output set . In this…","fields":{"slug":"/Parkinson_2/"},"frontmatter":{"date":"September 08, 2021","title":"Using a deep recurrent neural network with EEG signal to detect Parkinson’s disease","tags":["Paper Review","EEG","Parkinson's disease"]},"rawMarkdownBody":"\r\n## `Annals of Translational Medicine (2020)`\r\n\r\n### pooling-based deep recurrent neural network (PDRNN)\r\n\r\n<img src=\"model.png\">\r\n\r\n### DRNN architecture\r\n\r\nThe RNN maps the input vector $$(x)$$ to the equivalent output set $$(y)$$. In this graph, the learning procedure is carried out in each time-step in the range of $$t=1\\:\\:to\\:\\:τ$$. The sharing states related to the variables of each node in the $$l^{th}$$ layer are updated as follows\r\n\r\n<img src=\"layer_update.png\">\r\n\r\n- $$x(t)$$: input in time-step $$t$$\r\n- $$y(t), y_{target}(t)$$: predicted and real output\r\n- $$h_l(t)$$: sharing states of layer $$l$$\r\n- $$a_l(t)$$: input of $$l^{th}$$ layer that is composed of\r\n    1. $$x(t)$$ or $$h_{l-1}(t)$$\r\n    2. $$b$$ (bias values)\r\n    3. $$h_{l}(t-1)$$\r\n\r\n⇒ Because of the shared features of the recurrent neural network, it is able to learn the iterated uncertainties of the prior time-steps\r\n\r\nvanilla RNN 대신 LSTM을 사용함으로써 overcome this problem by generating paths through which the gradient is able to flow in long periods\r\n\r\n- stratified 10-fold cross validation\r\n- Single Sign On (SSO) optimizer\r\n- 1E-4 training rate\r\n- activation functions such as Relu in all layers\r\n- softmax in the last layer\r\n- dropout is adjusted to 0.5 in the dropout layer\r\n\r\n### Parkinson's and healthy cases\r\n\r\n20 PD patients (10 males and 10 females)\r\n\r\n- age range between 45 and 65 years old\r\n- average period of PD of 5.75 ± 3.52 years\r\n    - ranging from 2 to 10 years\r\n        - Phase 1: two patients\r\n        - Phase 2: eleven patients\r\n        - Phase 3: seven patients\r\n- The obtained Mini-Mental State Examination (MMSE) results were **within the range of the typical boundaries** of 25 to 30 (26.9 ± 1.51)\r\n\r\n20 healthy cases with equal age range (9 males and 11 females) without past a record of neurological (or mental) disorders were also examined\r\n\r\n- The MMSE results obtained from the healthy cases were in the range of 27.15 ± 1.63 years.\r\n\r\n### Preprocessing phase and EEG signals\r\n\r\n5 minutes in the steady state with a 128-Hz sample rate\r\n\r\nemotive EPOC neuroheadset with 14 channels\r\n\r\nthe recorded signals were divided into 2-second window lengths\r\n\r\nthreshold method for eliminating the signals at a level higher than ±100 μV (in order to eliminate\r\nthe eye-blinking effects)\r\n\r\nfrequencies were filtered using a Butterworth six-order band-pass filter with the forward-reverse method\r\n\r\n### PDRNN analysis\r\n\r\n- Accuracy: 88.57%\r\n- Precision: 88.31%\r\n- Sensitivity: 84.84%\r\n- Specificity: 91.81%\r\n\r\n**performance of the model**\r\n\r\n- with dropout layer\r\n\r\n<img src=\"performance_1.png\" width=\"500px\">\r\n\r\n- without dropout layer\r\n\r\n<img src=\"performance_2.png\" width=\"500px\">\r\n\r\n→ Remarkably, it was possible for over-fitting to occur in the model without a dropout layer.\r\n\r\n<img src=\"confusion_matrix.png\" width=\"500px\">\r\n\r\n### Discussion\r\n\r\n- 기존 연구와의 비교\r\n- 향후목표\r\n\r\n<img src=\"purpose.png\">\r\n\r\n- The main novelties of the proposed method are summarized as follows\r\n    1. A deep RNN architecture equipped with LSTM units can automatically detect PD using EEG\r\n    signals\r\n    2. The extraction, selection, and classification of the features is not required\r\n    3. A stratified 10-fold cross validation method is used for authentication of the model\r\n    4. This is the first time a deep learning method has been proposed to diagnose PD using EEG signals\r\n    5. Good diagnostic efficiency could be achieved even with a low number of healthy and PD cases, which shows good robustness of the proposed model\r\n- Nevertheless, our proposed method and study may also exhibit the following limitations\r\n    1. A low number of cases (20 healthy and 20 PD cases) were used for developing the proposed PDRNN model\r\n    2. The proposed PDRNN model may have high computing costs in comparison with the traditional machine learning methods\r\n\r\n---\r\n\r\n## 참고 자료\r\n\r\n- 논문: [Using a deep recurrent neural network with EEG signal to detect Parkinson’s disease](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7396761/)\r\n"},{"excerpt":"Proposed CNN architecture stratified tenfold cross-validation Adam optimization with a learning rate of 0.0001 activation functions such as…","fields":{"slug":"/Parkinson_1/"},"frontmatter":{"date":"September 07, 2021","title":"A deep learning approach for Parkinson's disease diagnosis from EEG signals","tags":["Paper Review","EEG","Parkinson's disease"]},"rawMarkdownBody":"\r\n## `Neural Computing and Applications (2018)`\r\n\r\n### Proposed CNN architecture\r\n\r\n<img src=\"model_1.png\" width=\"700px\">\r\n<img src=\"model_2.png\">\r\n\r\n- stratified tenfold cross-validation\r\n- Adam optimization with a learning rate of 0.0001\r\n- activation functions such as Relu for all layers\r\n- softmax for the last layer\r\n- dropout is set to 0.5\r\n\r\nthe kernel size and number of filters are obtained through the **brute force technique**\r\n\r\n### Subjects\r\n\r\n20 PD patients (10 women and 10 men)\r\n\r\n- age range between 45 and 65 years old\r\n- average period of PD of 5.75 ± 3.52 years\r\n    - ranging from 1 to 12 years\r\n        - Stage (i) - 2\r\n        - Stage (ii) - 11\r\n        - Stage (iii) - 7\r\n- MNSE(mini-mental status examination) results: observed to be **inside the range of the normal limits** [26.90 ± 1.51 (range 25-30)]\r\n\r\n20 normal subjects in the same age group (9 men and 11 women)\r\n\r\n- age of 58.10 ± 2.95\r\n- with no past record or indications of neurological or mental disorder were enlisted\r\n- MNSE results: 27.15 ± 1.63 years\r\n\r\n### EEG recordings and pre-processing\r\n\r\nlasted 5 min in resting state (to attain a state of relaxed wakefulness) at **128 Hz sampling rate**\r\n\r\nemotive EPOC neuroheadset of **14 channels**\r\n\r\nthe signals were **segmented into 2-s window length**\r\n\r\nA **threshold technique** was used to discard the signal amplitudes exceeding ± 100 $\\mu$V **to remove the eye blinking artifacts**\r\n\r\n**6th-order bandpass Butterworth filter with forward reverse filtering technique** was employed to filter the frequency range of 1-49 Hz\r\n\r\n### Results\r\n\r\n- Accuracy: 88.25%\r\n- Sensitivity: 84.71%\r\n- Specificity: 91.77%\r\n\r\n**performance of the model**\r\n\r\n- with dropout layer\r\n\r\n<img src=\"performance_1.png\" width=\"500px\">\r\n\r\n- without dropout layer\r\n\r\n<img src=\"performance_2.png\" width=\"500px\">\r\n\r\n→ without the dropout layer, there is a possibility of overfitting of data\r\n\r\n<img src=\"confusion_matrix.png\" width=\"500px\">\r\n\r\n### Discussion\r\n\r\n- 기존 연구와의 비교\r\n\r\n<img src=\"comparison.png\">\r\n\r\n- 향후목표\r\n\r\n<img src=\"purpose.png\">\r\n\r\n- 장단점\r\n    - advantages\r\n        1. A thirteen-layer CNN model is designed to automatically identify PD using EEG signals.\r\n        2. Extraction, selection, and classification of features are not required in the proposed CNN model.\r\n        3. The model is validated with a stratified tenfold crossvalidation technique.\r\n        4. This is the first work to implement the deep learning technique for the detection of PD using EEG signals.\r\n        5. It obtains good performance even with less number of normal and PD subjects. Hence, the developed is robust.\r\n    - disadvantages\r\n        1. It uses limited number of (20 normal and 20 PD) subjects to develop the CNN model.\r\n        2. The CNN structure is computationally expensive as compared to the conventional machine learning techniques.\r\n\r\n---\r\n\r\n## 참고 자료\r\n\r\n- 논문: [A deep learning approach for Parkinson’s disease diagnosis from EEG signals](https://link.springer.com/article/10.1007/s00521-018-3689-5)\r\n"},{"excerpt":"TabNet 1. Abstract Sequential attention을 사용해서 각각의 decision step마다 여러 feature들 중에 중요한 feature들만 고름 tabular data의 self-supervised learning…","fields":{"slug":"/TabNet/"},"frontmatter":{"date":"June 08, 2021","title":"TabNet: Attentive Interpretable Tabular Learning","tags":["Paper Review","Tabular Data","TabNet"]},"rawMarkdownBody":"\r\n## TabNet\r\n\r\n<img src=\"tabnet.PNG\">\r\n\r\n## 1. Abstract\r\n\r\n- Sequential attention을 사용해서 각각의 decision step마다 여러 feature들 중에 중요한 feature들만 고름\r\n- tabular data의 self-supervised learning도 가능함 (label이 안되어있는..)\r\n\r\n## 2. Related work\r\n\r\n### 1. Feature selection\r\n\r\n- global methods: feature importance를 기반으로 하여 feature selection\r\n- Instance-wise feature selection: response variable과 선택된 feature들간의 공통 정보를 최대화 하기 위해 explainer model을 사용해서 feature를 individually하게 뽑음\r\n- TabNet: soft feature selection with controllable sparsity in end-to-end learning  \r\n  → 단일 모델이 feature selection을 하고 output mapping까지 하는 구조\r\n\r\n### 2. Tree-based learning\r\n\r\n- tabular data를 위해 많이 사용\r\n- variance를 줄이기 위해 앙상블 기법을 사용  \r\n  → random forests, XGBoost, LightGBM 등이 있음\r\n- 딥러닝이 feature selecting property를 retrainig 시키면 성능이 더 오를 수 있다고 주장\r\n\r\n### 3. Integration of DNNs into DTs\r\n\r\n### 4. Self-supervised learning\r\n\r\n<img src=\"self-supervised learning.png\">\r\n\r\n- small data에서 unsupervised representation learning이 성능을 향상시킴을 보임\r\n- 최근 연구에서는 text와 image에서 큰 성능 향상의 폭을 보임\r\n  - unsupervised learning과 딥러닝에서 현명한 masking을 통해서 학습을 시키면 좀 더 오름\r\n\r\n## 3. Unsupervised pre-training vs Supervised fine-tuning\r\n\r\n<img src=\"self-supervised learning.png\">\r\n\r\n- Unsupervised Pre-training은 Mask를 씌우고 Mask를 씌운 부분을 맞추는 방법\r\n    - Label이 따로 필요 없음\r\n    - 엄청난 loss...\r\n- Supervised fine-tuning은 다 채워진 tabular data를 가지고 Decision making을 하는 방법\r\n\r\n## 4. Tabnet 특징\r\n\r\n1. Data에서 Sparse instance-wise feature selection을 사용\r\n  - salient features(두드러진 feature)들을 선택(selection)하는 것은 높은 성능을 내는데 중요한 요소(특히 dataset이 작을 때)\r\n  - 따라서 dataset에서 subset을 만들어 determine(결정)하도록 재구성함\r\n\r\n  <img src=\"feature_subset.PNG\">\r\n\r\n  - 위의 표와 같이 feature에 따른 subset을 만든 것이 Syn1 ~ Syn6 까지임\r\n  - 여기서 Syn1 ~ Syn3까지의 값은 feature들끼리 독립적인 feature들을 사용\r\n    - 이렇게 사용하면 global feature (성능이 가장 잘 나오는 이상적인 feature)에 가까워짐\r\n  - 하지만 Syn4 ~ Syn6까지는 feature들끼리의 종속성이 존재함\r\n    - instance끼리의 wise한 feature들\r\n    - 이렇게 사용하면 global selection이 suboptimal로 선택되면서 성능이 낮아질 수 있음\r\n  - 하지만 Tabnet은 instance-wise feature들을 sparse하게 사용하면서 global feature에 가까워지게 만들어줌\r\n\r\n2. Sequential한 multi-step architecture을 가지고 있음\r\n\r\n- 각 step들은 선택된 feature들을 가지고 decision을 내리는데 도움을 줌\r\n\r\n3. feature 선택에 있어 non-linear한 processing을 사용하면서 learning capacity를 향상시킴\r\n\r\n## 5. Tabnet 전체 Architecture\r\n\r\n<img src=\"tabnet_encoder.PNG\">\r\n\r\n<img src=\"tabnet_decoder.PNG\">\r\n\r\n- 전체적으로는 Tabnet의 Encoder 부분과 Decoder 부분으로 구성됨\r\n- Encoder 부분은 Sequential한 multi-setp processing ($$N_{steps}$$ decision steps로 구성됨)\r\n- Decoder는 self-supervised learning을 할 때만 사용함\r\n\r\n## 6. Feature selection\r\n\r\n<img src=\"learnable_mask.png\">\r\n\r\n- Learnable Mask $$M[i] \\in R^{B \\times D}$$를 사용\r\n    - salient feature들을 selection 하기 위해 사용함\r\n\r\n- **Attentive transformer 사용**\r\n\r\n  <img src=\"attentive_transformer.PNG\">\r\n\r\n  - 이전 단계에서 처리된 features들을 사용하여 mask를 얻는 방법\r\n  - $$M[i] = sparsemax(P[i-1] \\cdot h_{i}(a[i-1]))$$\r\n    - $$a[i-1]$$: 이전 단계에서 처리된 feature\r\n    - $$h_{i}$$: trainable function\r\n      - FC layer와 BN을 의미함\r\n    - $$P[i]$$: prior scale term\r\n      - 특정한 feature가 이전에 얼마나 사용되었는지를 나타냄\r\n      - $$P[i] = \\prod^{i}_{j=1} ( \\gamma - M[j] )$$\r\n        - $$\\gamma$$는 relaxation parameter\r\n          - $$\\gamma = 1$$일 때, feature가 한 decision step에서 한개만 사용하도록 함\r\n          - $$\\gamma$$가 증가하면 여러 decision step에서 feature들을 사용할 수 있음\r\n        - $$P[0]$$이면 $$1^{B \\times D}$$로 초기화 (모두 $$1$$로 초기화)\r\n          - 1이면 feature를 사용한다는 의미이므로, feature를 사용하지 않을 때 $$P[0]$$을 $$0$$으로 만들어줌\r\n\r\n## 7. Feature processing\r\n\r\n<img src=\"feature_processing.PNG\">\r\n\r\n- **Feature transformer 사용**\r\n\r\n  <img src=\"feature_transformer.PNG\">\r\n\r\n  - 대용량을 처리할 때도 robust한 learning을 만들기 위해서는, feature transformer는 `모든 decision step에서 공유되는 layer`와 `decision step-dependents layer`로 구성\r\n    - `모든 decision step에서 공유되는 layer`를 쓰는 이유는 같은 feature들 모두 다른 decision step의 input으로 들어가기 때문\r\n    - `decision-step-dependents layer`는 해당 decision일 때만 사용하는 feature들을 processing하는 부분\r\n  - 위 사진에서는 다음과 같이 구성됨\r\n    - 두 개의 shared layer\r\n    - 두 개의 decision step-dependent layers\r\n  - FC → BN → GLU로 연결\r\n    - 밑에 있는 화살표는 normalized residual connection\r\n    - Residual connection을 $$\\sqrt{0.5}$$로 정규화하는 이유는 network 전체의 분산이 크게 변화지 않게 함으로써 학습 안정화에 도움이 됨 (Gehring et al. 2017)\r\n  - 빠른 training을 위해 BN과 함께 large batch size를 사용해도 됨\r\n    - ghost BN..?\r\n  - Aggregation\r\n    - 모든 decision을 embedding $$d[i]$$\r\n    - $$d_{out} = \\sum^{N_{steps}}_{i=1} RELU(d[i])$$\r\n    - 이후, 마지막 linear를 태움: $$W_{final} d_{out}$$\r\n\r\n- decision step의 output을 split함\r\n\r\n## 8. Interpretability (해석 가능성)\r\n\r\n- tabnet의 feature selection mask는 선택된 feature에서 강조표시를 할 수 있음\r\n  - 일단, $$M_{b,j}[i] = 0$$이면, decision에 참여할 권한 없음\r\n  - $$M_{b,j}[i]$$ 계수는 feature importance를 나타내는 $$f_{b,j}$$라고 볼 수 있음.\r\n    ($$f_{i}$$가 linear function일 때)\r\n    - 그렇지만 각각의 decision step이 non-linear하다고 하더라도, output을 linear하게 합치면 되기 때문에 상관없음\r\n  - 서로 다른 단계에 있는 mask를 합치려면, decision의 각 단계에서 상대적 중요도를 평가할 수 있는 계수가 필요\r\n    - 논문에서는 $$\\eta b[i] = \\sum^{N_{d}}_{c=1} RELU(d_{b,c}[i])$$\r\n      - 즉, $$i^{th}$$번째 decision과 $$b^{th}$$의 sample들을 aggregate한 decision\r\n    - 위 식을 사용해서 aggregate feature importance mask를 찾는 방법은 다음과 같음\r\n      - $$M_{agg-b,j} = \\frac {\\sum^{N_{steps}}_{i=1} \\eta b[i]M_{b,j}[i]} {\\sum^{D}_{j=1} \\sum^{N_{steps}}_{i=1} \\eta b[i] M_{b,j}[i]}$$\r\n\r\n## 9. self-supervised learning\r\n\r\n<img src=\"tabnet_decoder.PNG\">\r\n\r\n- Self-supervised learning을 하기 위해 Decoder를 사용\r\n- Decoder는 feature transformers, FC를 사용함\r\n- mask를 씌운 부분을 맞추는 것이기 때문에 binary mask인 $$S \\in \\{0,1\\}^{B \\times D}$$를 사용\r\n- Tabnet encoder에는 $$(1 - S) \\cdot \\hat{f}$$를 input으로 넣음  \r\n  Tabnet decoder는 reconstructed featres인 $$S \\cdot \\hat{f}$$를 출력함  \r\n- Reconstruction loss는 다음과 같이 계산됨\r\n\r\n<img src=\"reconstruction_loss.png\">\r\n\r\n- We sample $$S_{b,j}$$ independently from a **Bernoulli distribution** with parameter $$p_{s}$$, at each iteration.\r\n\r\n---\r\n\r\n## 결과\r\n\r\n- Regression과 Classification task를 수행할 수 있음\r\n- Forest Cover Type Dataset에서 높은 accuracy를 보여줌\r\n\r\n<img src=\"result.png\">\r\n\r\n<img src=\"result2.png\">\r\n\r\n- Pocker Hand dataset에서도 좋은 성능을 보여줌\r\n\r\n<img src=\"result3.png\">\r\n\r\n<img src=\"result4.png\">\r\n\r\n- Self-supervised learning에서 Dataset size가 크고, pre-training을 함께 진행했을 때 좋은 성능이 나오게됨\r\n\r\n<img src=\"result5.png\">\r\n\r\n## Hyperparameter 설정 방법\r\n\r\n$$N_d, N_a = \\{8, 16, 24, 32, 64, 128\\}$$  \r\n$$N_{steps} = \\{3, 4, 5, 6, 7, 8, 9, 10\\}$$  \r\n$$\\gamma = \\{1.0, .12, 1.5, 2.0\\}$$  \r\n$$\\lambda_{sparse} = \\{0, 0.000001, 0.0001, 0.001, 0.01, 0.1\\}$$  \r\n$$B = \\{256, 512, 1024, 2048, 4096, 8192, 16384, 32768\\}$$  \r\n$$B_V = \\{256, 512, 1024, 2048, 4096\\}$$  \r\n$$lr = \\{0.005, 0.01, 0:02, 0.025\\}$$  \r\n$$decay \\ rate = \\{0.4, 0.8, 0.9, 0.95\\}$$  \r\n\r\n- **Guidelines for hyperparameters**\r\n  - $$N_{steps}$$는 [3, 10] 사이가 optimal하다.\r\n    - 너무 높은 $$N_{steps}$$를 사용하면 overfitting의 문제가 생길 수 있다\r\n  - $$N_d, N_a$$의 설정 문제는 performance와 complexity의 trade-off 문제를 겪을 수 있다.\r\n    - 따라서 $$N_a = N_d$$가 가장 이상적인 choice이다.\r\n  - $$\\gamma$$는 Tabnet의 hyperparameter에서 높은 성능을 낼 수 있는 중요한 역할\r\n    - 대부분 larger $$N_{steps}$$를 사용하면 larger $$\\gamma$$를 사용해야한다.\r\n  - larger batch size를 사용하는 것이 이득이다.\r\n  - large learning rate를 쓰는 것이 중요하다.\r\n    - 어차피 decay될 것\r\n\r\n---\r\n\r\n# 평가\r\n\r\n- tabular data를 처리할 수 있는 딥러닝\r\n- unsupervised 방식도 가능함\r\n\r\n---\r\n\r\n## 참고 자료\r\n\r\n- 논문: [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\r\n- GitHub: [TabNet](https://github.com/dreamquark-ai/tabnet)\r\n"},{"excerpt":"Transformer-DST 1. Key Idea 이전 논문들에서는 Encoder에서 BERT를 사용하지만, Value Generation부분에서는 RNN Decoder를 사용하는 아이러니.. Purely Transformer-based…","fields":{"slug":"/Transformer-DST/"},"frontmatter":{"date":"May 12, 2021","title":"Jointly Optimizing State Operation Prediction and Value Generation for Dialogue State Tracking","tags":["Paper Review","DST","Dialogue State Tracking","Transformer-DST"]},"rawMarkdownBody":"\r\n## Transformer-DST\r\n\r\n## 1. Key Idea\r\n\r\n- 이전 논문들에서는 Encoder에서 BERT를 사용하지만, Value Generation부분에서는 RNN Decoder를 사용하는 아이러니..\r\n- Purely Transformer-based framework를 사용\r\n즉, Single BERT가 **Encoder와 Decoder 모두**에서 work\r\n→ 이렇게 하면 prediction objective와 value generation objective가 BERT 하나만 optimize하게 됨\r\n- Encoder(BERT)에서 사용한 hidden states 값을 Decoder(BERT)에서 Re-use가 가능해짐\r\n\r\n## 2. Input\r\n\r\n- Input으로는 SOM-DST와 같은 구조를 지님\r\n- 아래 그림과 같이 **D1, D2, S1을 INPUT**으로 넣고, **OUTPUT으로 S2**를 출력\r\n\r\n<img src=\"input.PNG\" height=150>\r\n\r\n- $$D_{t-1}, D_{t}, S_{t-1}$$을 INPUT으로 $$S_{t}$$를 PREDICT\r\n- $$D_{t}$$ : t번째 dialogue turn의 (System utterance, User response) pair\r\n- $$S_{t}$$ : $$(d_{j}, s_{j}, v_{j}) | 1<=j<=J$$\r\n\r\n여기서 $$d$$는 domain, $$s$$는 slot, $$v$$는 value를 나타냄\r\n\r\n만약, 아무 정보도 없을시 $$(d_{j}, s_{j})$$로 나타내고, $$v_{j}$$는 NULL\r\n\r\n## 3. Overview\r\n\r\n- 왼쪽은 Transformer Encoder, 오른쪽은 Transformer Decoder\r\n- Encoder (왼쪽)에서 $$h_{sl}^{L}$$ (hidden state)를 뽑아내고, Decoder (오른쪽)에서 Re-use하는 방법으로 사용됨\r\n- Decoder (오른쪽)은 left-to-right attention (왼쪽에서 오른쪽으로 차례로 출력하는 language model, 즉 왼쪽 출력값이 오른쪽 입력)\r\n\r\n<img src=\"overview.PNG\">\r\n\r\n## 4. Encoder\r\n\r\n<img src=\"encoder.PNG\">\r\n\r\n- Encoder의 Input은 $$D_{t-1}, D_{t}, S_{t-1}$$ 3가지가 들어감\r\n    - $$D_{t}$$는 t번째 turn의 ( System Utterance, User Resposne ) pair\r\n    - $$S_{t-1}$$은 $$(d_{j}, s_{j}, v_{j}) | 1<= j <= J$$\r\n\r\n### 4-1) Encoder Input\r\n\r\n<img src=\"encoder_input.PNG\">\r\n\r\n- $$[SLOT] \\bigoplus d_{j} \\bigoplus - \\bigoplus s_{j} \\bigoplus - \\bigoplus v_{j}$$ 으로 구성\r\n    - $$\\bigoplus$$는 concat을 나타냄\r\n    - 총 $$J$$개의 domain-slot에 대해서 만들어줌\r\n    - $$[SLOT] \\bigoplus d_{j} \\bigoplus - \\bigoplus s_{j} \\bigoplus - \\bigoplus v_{j}$$  * J번\r\n- $$[SLOT]$$은 Transformer block을 통과한 후, $$X^{l}_{sl_{j}}$$ 형태로 출력되며, Prediction (CARRYOVER, ..., UPDATE 등)으로 사용됨\r\n\r\n### 4-2) Multi-head self-attention\r\n\r\n<img src=\"multi-head.png\">\r\n\r\n- Multi-head Self-attention 매카니즘 사용\r\n\r\n<img src=\"multi-head2.png\">\r\n\r\n- 여기서 $$M^{x}$$ : self-attention mask matrix\r\n    - $$M^{x} \\in R^{|x| \\times |x|}$$\r\n    - $$M^{x}_{ij} \\in \\{0, - \\infty \\}$$\r\n    - $$M^{x}_{ij} = 0$$이면 i-th position이 j-th position에 attend하다는 의미\r\n    - $$M^{x}_{ij} = -\\infty$$이면 i-th position과 j-th position을 prevents하겠다는 의미\r\n\r\n### 4-3) Encoder Output\r\n\r\n$$X^{L} = [x^{L}_{cls}, x^{L}_{1}, ..., x^L_{sl_{1}}, ..., x^{L}_{sl_{J}},...]$$\r\n\r\n### 4-4) Encoder Objective\r\n\r\n- Encoder outputs $$x^{L}_{sl_{j}}$$에서 $$[SLOT]$$칸에 해당하는 값을 확인\r\n- CARRYOVER, DELETE, DONTCARE, UPDATE\r\n- UPDATE의 경우에만 decoder generater에서 사용함\r\n\r\n## 5. Decoder (Slot Value Generation)\r\n\r\n<img src=\"decoder.PNG\">\r\n\r\n- Left-to-right self-attention을 사용함\r\n- Encoder에서 도출해낸 hidden states를 decoder에서 reuse\r\n- Resue의 의미는 hidden state를 decoder에서 다시 한번 계산할 필요가 없어진다는 의미를 갖음.\r\n\r\n### 5-1) Decoder Input\r\n\r\n<img src=\"decoder_input.PNG\">\r\n\r\n- Encoder (reuse)\r\n    - 왼쪽의 $$D_{t}$$와 $$[SLOT]$$은 Encoder 부분을 나타냄\r\n    - 현재 turn의 $$D_{t}$$의 hidden state vector를 사용함\r\n    - $$[SLOT]$$ 중 UPDATE로 prediction이 된 hidden state vector만 사용함\r\n- Decoder\r\n    - $$[BOS]$$는 String의 시작\r\n    - $$w^{v_{j}}_1, w^{v_{j}}_2$$는 decoder의 output을 다시 input으로 가지고 와서 사용 (left-to-right self-attention)\r\n\r\n### 5-2) Left-to-right self-attention\r\n\r\n<img src=\"left-to-right.png\">\r\n\r\n<img src=\"left-to-right2.png\">\r\n\r\n- 일반 Multi-head attention하고 비슷\r\n- $$\\hat{X}$$ : re-used된 encoder hidden states\r\n- $$Y$$ : Decoder hidden states\r\n- $$\\hat{X}$$와 $$Y$$를 concat해서 사용\r\n- 만약 $$j \\leq i$$일때,  $$M^{y}_{ij} = 0$$ 으로 사용 (left-to-right attention)\r\n\r\n### 5-3) Decoder Objective\r\n\r\n- **Generated slot value loss**와 **ground-truth slot value**를 비교해서  Loss를 산출\r\n- Teacher Forcing을 모든 time에서 사용\r\n\r\n# 결과\r\n\r\n- MutliWOZ 2.0과 MultiWOZ 2.1에서 제출 당시 SOTA\r\n\r\n<img src=\"result.png\">\r\n\r\n- 각각의 Domain의 Joint goal Accuracy를 비교\r\n    - 신기한 점은 Taxi 빼고 (다른 모델보다) 높은 성능을 보임\r\n    - 이유를 찾아보니 Taxi의 경우 Train과의 co-occurrence relations가 있음\r\n    - 하지만 Ours에는 이러한 점을 해결하려고 하지는 않았음\r\n\r\n<img src=\"result2.png\">\r\n\r\n- 시간의 경우 SOM-DST보다는 Inference time이 오래걸림\r\n\r\n<img src=\"result3.png\">\r\n\r\n- Reuse를 사용했을 경우에도 여러가지 방법을 시도해보았고, $$D_{t} + [SLOT]$$을 사용했을 때 Joint Accuracy에서 좋은 성능을 보였음\r\n\r\n<img src=\"result4.png\">\r\n\r\n---\r\n\r\n## 참고 자료\r\n\r\n- 논문: [Jointly Optimizing State Operation Prediction and Value Generation for Dialogue State Tracking](https://arxiv.org/abs/2010.14061)\r\n- GitHub: [Transformer-DST](https://github.com/zengyan-97/Transformer-DST)\r\n"},{"excerpt":"SOM-DST SOM-DST 기존 모델의 문제점 Ontology-based DST 실제 시나리오에 잘 대응하지 못함 unseen value를 처리할 수 없음 ontology가 많으면 처리 시간이 오래 걸림 Open-vocab-based DST…","fields":{"slug":"/SOM-DST/"},"frontmatter":{"date":"May 10, 2021","title":"Efficient Dialogue State Tracking by Selectively Overwriting Memory","tags":["Paper Review","DST","Dialogue State Tracking","SOM-DST"]},"rawMarkdownBody":"\r\n## SOM-DST\r\n\r\n![SOM-DST](https://github.com/clovaai/som-dst/raw/master/img/overview6.png)\r\n\r\n## 기존 모델의 문제점\r\n\r\n### Ontology-based DST\r\n\r\n- 실제 시나리오에 잘 대응하지 못함\r\n- unseen value를 처리할 수 없음\r\n- ontology가 많으면 처리 시간이 오래 걸림\r\n\r\n### Open-vocab-based DST (TRADE)\r\n\r\n- turn마다 slot의 모든 value를 생성해야해서 비효율적임\r\n\r\n---\r\n\r\n## Definition\r\n\r\n$$t$$: turn\r\n\r\n$$S^j$$: slot\r\n\r\n$$V^j_t$$: corresponding slot value\r\n\r\n$$J$$: total number of such slots\r\n\r\n$$r^j_t\\in O = \\left\\{\\rm\\textbf{CARRYOVER, DELETE, DONTCARE, UPDATE}\\right\\}$$\r\n\r\n$$A_t$$: System response\r\n\r\n$$U_t$$: User utterance\r\n\r\n---\r\n\r\n## State Operation Predictor (Encoder)\r\n\r\nEncoder 모델로 **pretrained BERT encoder** 사용\r\n\r\n### <span style=\"background-color:#fcffb0\">Encoder Input을 만들기 위한 준비물</span>\r\n\r\n$$D_t = A_t ⊕ ; ⊕ U_t ⊕ [SEP]$$: dialogue utterances at turn t\r\n\r\n- `;` $A_t$와 $U_t$를 구분하기 위한 스페셜 토큰\r\n- `[SEP]` dialogue turn이 끝났다는 것을 표시하기 위한 스페셜 토큰\r\n\r\n$$B_t^j = [SLOT]^j ⊕ S^j ⊕ - ⊕ V_t^j$$ : representation of the j-th slot-value pair\r\n\r\n- j-th slot-value pair를 하나의 벡터로 aggregate\r\n- $$`[SLOT]^j`$$\r\n\r\n    [SLOT] 이라는 스페셜 토큰을 사용\r\n\r\n    BERT의 [CLS] 토큰과 같은 역할\r\n\r\n- $$`V_t^j`$$\r\n\r\n    <img src=\"V_t^j.PNG\" height=150>\r\n\r\n$$B_t = B_t^1 ⊕ ... ⊕ B_t^J$$ : representation of the dialogue state at turn t\r\n\r\n### Encoder Input\r\n```\r\n$$X_t = [CLS] ⊕ D_{t-1} ⊕D_t ⊕ B_{t-1}$$\r\n\r\nsegment id:        0       1         1\r\n```\r\n⇒ **Input : Sum($X_t$ embedding, segment id embedding, positional embedding)**\r\n\r\ndialogue history로 이전 턴의 dialogue utterances $$D_{t-1}$$을 사용한다.\r\ndialogue history의 size: 1\r\n\r\n모델이 입력으로 들어오는 dialogue 간의 `Markov property`를 가정\r\n\r\n이전 turn dialogue state $$B_{t-1}$$은 전체 dialogue history를 압축적으로 표현하는 역할\r\n\r\n### Encoder Output\r\n\r\n$$H_t \\in \\mathbb R^{\\left|X_t\\right|\\times d}$$ : $$h_t^{\\rm X}$$ $$(t=1...t)$$ 까지 집합\r\n\r\n$$h_t^{[CLS]}, h_t^{[SLOT]^j} \\in \\mathrm R^d$$\r\n\r\n- $$[CLS], [SLOT]^j$$에 대응하는 output\r\n\r\n$$h_t^X = tanh(W_{pool} h_t^{[CLS]})$$\r\n\r\n- $$h_t^X$$: 전체 input $$X_t$$를 sequence로 aggregate\r\n- $$W_{pool} \\in \\mathbb R^{d\\times d}$$: feed-forward layer with a learnable parameter\r\n\r\n### State Operation Prediction\r\n\r\n$$P^j_{opr, t} = softmax(W_{opr} h_t^{[SLOT]^j})$$\r\n\r\n- $$W_{opr} \\in \\mathbb R^{\\left| O\\right|\\times d}$$ : learnable parameter\r\n- $$P_{opr, t}^j \\in \\mathbb R^{\\left| O\\right|}$$ : j-th slot의 turn t에서의 연산에 대한 확률 분포\r\n- SOM-DST에서는 $$\\left| O\\right| = 4$$,\r\n\r\n    $$O = \\left\\{\\rm{CARRYOVER, DELETE, DONTCARE, UPDATE}\\right\\}$$\r\n\r\n→ $$r_t^j = argmax(P_{opr, t}^j)$$\r\n\r\n→ slot의 Operation의 결과가 `UPDATE` 일 때 slot value를 generation\r\n\r\n- Encoder에서 나온 Operation의 결과가 `Update`인 경우를 집합으로 표현하면\r\n\r\n    $$\\mathbb{U}_t = \\left\\{j|r_t^j = \\rm{UPDATE}\\right\\}$$, and its size as $$J_t^\\prime = \\left| \\mathbb{U}_t\\right|$$\r\n\r\n    Recab for V\r\n\r\n    <img src=\"V_t^j.PNG\" height=150>\r\n\r\n---\r\n\r\n## Slot Value Generator (Decoder)\r\n\r\n- Encoder에서 나온 Operation의 결과가 `Update` 인 경우 해당 slot의 value를 예측\r\n- SOM-DST의 generator는 value를 $$J$$개가 아닌 $$J^\\prime_t$$개의 slot에 대해서만 만들어준다.\r\n\r\n    대부분의 경우에서 $$J^\\prime_t \\ll J$$이기 때문에 더 효율적이라고 주장\r\n\r\n- Decoder 모델로 **GRU** 사용\r\n    - 입력으로 word embedding vector $$e_t^{j,k}$$를 받으면서 **GRU**의 hidden state vector $$g_t^{j, k}$$를 recurrent하게 업데이트\r\n    - $$g_t^{j, 0} = h_t^{\\rm x}$$, $$e_t^{j,0} = h_t^{[slot]j}$$: **GRU**에 들어가는 초기값\r\n    - $$g_t^{j, k} = GRU(g_t^{j, k-1}, e_t^{j,k})$$\r\n    - $$e_t^{j,k}$$가 [EOS] 토큰이 나올때까지 진행\r\n    - hidden state $$g_t^{j, k}$$는 k-th decoding step을 거치면서 vocabulary 와 user utterance의 단어에 대한 확률 분포로 변함\r\n\r\n        $$P^{j, k}_{vcb, t} = softmax(Eg^{j, k}_t) \\in \\mathbb R^{d_{vcb}}$$\r\n\r\n        - $$E \\in \\mathbb R^{d_{vcb}\\times d}$$: Encoder와 Decoder가 서로 공유하는 word embedding matrix\r\n            - $$d_{vcb}$$: vocabulary size\r\n\r\n        $$P^{j, k}_{ctx, t} = softmax(H_t g_t^{j, k}) \\in \\mathbb R^{\\left|X_t\\right|}$$\r\n\r\n        - user utterance의 단어에 대한 확률 분포\r\n\r\n        $$P^{j, k}_{val, t} = \\alpha P^{j, k}_{vcb, t} + (1-\\alpha) P^{j, k}_{ctx, t}$$: final output distribution\r\n\r\n        - $$\\alpha = sigmoid(W_1 \\left[g^{j, k}_t ; e^{j, k}_t ; c^{j, k}_t\\right])$$\r\n            - $$W_1 \\in \\mathbb R^{1\\times (3d)}$$: learnable parameter\r\n            - $$c^{j, k}_t = P^{j, k}_{ctx, t} H_t \\in \\mathbb R^d$$: context vector\r\n\r\n---\r\n\r\n## Objective Function\r\n\r\n### State operation predictor\r\n\r\n**Main Task**\r\n\r\nstate operation classification\r\n\r\n**Auxiliary Task**\r\n\r\ndomain classification\r\n\r\nstate operation classification 외에도 domain classification을 보조 task로 사용하여 모델이 dialogue turn 간의 slot operation과 domain transition의 상관 관계를 학습하도록 함\r\n\r\n$$P_{dom, t} = softmax(W_{dom} h_t^{\\rm X})$$\r\n\r\n- $$W_{dom} \\in \\mathbb R^{d_{dom}\\times d}$$: learnable parameter\r\n- $$P_{dom, t} \\in \\mathbb R^{d_{dom}}$$: turn t에서 domain에 대한 확률 분포\r\n    - $$d_{dom}$$: # of domains defined in the dataset\r\n\r\n**Average of the negative log-likelihood**\r\n\r\n$$L_{opr, t} = -\\frac{1}{J}\\sum_{j=1}^{J}(Y_{opr, t}^j)^\\top log(P^j_{opr, t})$$\r\n\r\n$$L_{dom, t} = -(Y_{dom, t})^\\top log(P_{dom, t})$$\r\n\r\n- $$Y_{dom, t} \\in \\mathbb R^{d_{dom}}$$: one-hot vector for the ground truth domain\r\n- $$Y^j_{opr, t} \\in \\mathbb R^{\\left| O\\right|}$$: one-hot vector for the ground truth operation for the j-th slot\r\n\r\n### Slot value generator\r\n\r\n**Average of the negative log-likelihood**\r\n\r\n$$L_{svg, t} = -\\frac{1}{\\left|\\mathbb U_t\\right|}\\sum_{j\\in\\mathbb U_t}^{}\\left[\\frac{1}{K^j_t}\\sum_{k=1}^{K^j_t}(Y_{val, t}^{j, k})^{\\top}log(P^{j, k}_{val, t})\\right]$$\r\n\r\n- $$K_t^j$$: # of tokens of the ground truth value that needs to be generated for the j-th slot\r\n- $$Y_{val, t}^{j, k} \\in \\mathbb R^{d_{vcb}}$$: one-hot vector for the ground truth token that needs to be generated for the j-th slot at the k-th decoding step\r\n\r\n### Final Loss\r\n\r\nto minimized $$L_{joint, t} = L_{opr, t} + L_{dom, t} + L_{svg, t}$$\r\n\r\n---\r\n\r\n## Experimental Setup\r\n\r\n### Datasets\r\n\r\nMultiWOZ 2.0 and MultiWOZ 2.1\r\n\r\n### Training\r\n\r\n- Encoder : Bert-base-uncased\r\n- Decoder : GRU\r\n- Hidden size : 768\r\n- Optimizer : BertAdam\r\n- Encoder LR and warmup : 4e-5, 0.1\r\n- Decoder LR and warmup : 1e-4, 0.1\r\n- Batch size : 32\r\n- Dropout : 0.1\r\n- Word Dropout 적용, 0.1확률로 word 를 [UNK] 로 바꿈\r\n- Input max length : 256\r\n- Training Epoch : 30\r\n\r\n---\r\n\r\n## 결과\r\n\r\n### Joint Goal Accuracy\r\n\r\n<img src=\"joint_goal_accuracy.PNG\" height=400>\r\n\r\n> † indicates the case where BERT-large is used for our model\r\n\r\n### Domain-specific Accuracy\r\n\r\n<img src=\"domain-specific_accuracy.PNG\" height=450>\r\n\r\n### Latency\r\n\r\n<img src=\"latency.PNG\" height=150>\r\n\r\n---\r\n\r\n## 평가\r\n\r\n- JGA, Domain-specific Accuracy 에서 SOTA 혹은 비슷한 수준의 성능을 보여줌\r\n- inferecnce 타임이 매우 짧음에도 불구하고 좋은 성능을 보여줌\r\n\r\n---\r\n\r\n## 참고 자료\r\n\r\n- 논문: [Efficient Dialogue State Tracking by Selectively Overwriting Memory](https://arxiv.org/abs/1911.03906)\r\n- 영상: [[Paper Review] SOM-DST : Efficient Dialogue State Tracking by Selectively Overwriting Memory - KoreaUniv DSBA](https://www.youtube.com/watch?v=7Nwe2BBUZsw)\r\n- GitHub: [SOM-DST](https://github.com/clovaai/som-dst)\r\n"},{"excerpt":"CHAN-DST slot imbalance 문제를 해결하고자 adaptive objective를 도입 a contextual hierarchical attention network (CHAN)를 사용: dislogue history에서 relevant…","fields":{"slug":"/CHAN-DST/"},"frontmatter":{"date":"May 07, 2021","title":"A Contextual Hierarchical Attention Network with Adaptive Objective for Dialogue State Tracking","tags":["Paper Review","DST","Dialogue State Tracking","CHAN-DST"]},"rawMarkdownBody":"\r\n## CHAN-DST\r\n\r\n- slot imbalance 문제를 해결하고자 adaptive objective를 도입\r\n- a contextual hierarchical attention network (CHAN)를 사용: dislogue history에서 relevant context를 찾기 위함  \r\n  → 각 턴의 발화로부터 word-level 관련 정보 검색  \r\n  → contextual representation으로 encode  \r\n  → 모든 context표현을 turn-level관련 정보로 집계한 후 word-level 정보와 합친 output 생성  \r\n- state transition prediction task\r\n\r\n---\r\n\r\n## Definition\r\n\r\n- $$T$$: turn\r\n- $$U_t$$: user utterance of turn t\r\n- $$R_t$$: system response of turn t\r\n- $$X$$: $$\\left\\{(U_1, R_1), ... , (U_T, R_T)\\right\\}$$\r\n- $$B_t$$: $$\\left\\{(s, v_t), s \\in S\\right\\}$$\r\n- $$S$$: set of slots\r\n- $$v_t$$: corresponding value of the slot $$s$$\r\n- slot: concatenation of a domain name and a slot name\r\n\r\n---\r\n\r\n## Contextual Hierarchical Attention Network\r\n\r\n<img src=\"structure.png\">\r\n\r\n### 1. Sentence Encoder\r\n\r\n<img src=\"sentence_encoder.png\">\r\n\r\n`utterance encoder`\r\n\r\n- BERT special token 사용  \r\n  → [CLS] : 문장의 representation들을 합치기위해 사용 (to aggregate the whole representation of a sentence)  \r\n  → [SEP] : 문장의 끝을 나타내기위해 사용.  \r\n- $$U_t = \\left\\{w_1^u, ..., w_l^u\\right\\}$$ (user utterance)  \r\n  $$R_t = \\left\\{w_1^r, ..., w_{l'}^r\\right\\}$$ (system response)  \r\n  $$t$$: dialogue turn  \r\n- $$h_t = BERT_{finetune}([R_t;U_t])$$  \r\n  ($$h_t$$: contextual word representations)  \r\n- 여기서 BERT finetune은 training도중 finetuning이 될것을 의미.\r\n\r\n`slot-value encoder`\r\n\r\n- $$BERT_{fixed}$$는 contextual semantics vectors로 encode해준다.\r\n- utterance encode할때와 다른 점은 [CLS] 토큰의 output vector를 전체 문장 representation할때 사용한다. (to obtain the whole sentence representation)\r\n- $$h_s = BERT_{fixed}(s)$$  \r\n  $$h_t^v = BERT_{fixed}(v_t)$$  \r\n- $$BERT_{fixed}$$는 training 도중 고정되어있다. 그래서 우리 모델은 unseen slots and values에 대해서 original BERT representation로 확장해서 보는게 가능하다.\r\n\r\n### 2. Slot-Word Attention\r\n\r\n<img src=\"slot-word_attention.png\">\r\n\r\n- slot-word attention은 multi-head attention을 사용한다.\r\n- $$c_{s,t}^{word} = MultiHead(h^s, h_t, h_t)$$\r\n\r\n### 3. Context Encoder\r\n\r\n<img src=\"context_encoder.png\">\r\n\r\n- context encoder : unidirectional transformer encoder\r\n- {1, ..., t} 턴에서 추출 된 word-level slot-related 정보의 contextual relevance를 모델링하기 위한 것.\r\n- $$N$$개의 idenctical한 layer가 있다.\r\n  - 각 layer는 2개의 sub-layer를 가지고 있다.\r\n  - 첫번째 sub-layer: masked multi-head self-attention(Q = K = V)\r\n  - 두번째 sub-layer: position-wise fully connected feed-forward network(FFN) (two linear transformations, RELU activation으로 구성)\r\n  - $$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$$\r\n- $$m^n = FFN(MultiHead(m^{n-1}, m^{n-1}, m^{n-1}))$$  \r\n  $$m^0 = [c_{s,1}^{word} + PE(1), ..., c_{s,t}^{word} + PE(t)]$$  \r\n  $$c_{s,\\leq t}^{ctx} = m^N$$  \r\n- $$m^n$$: n번째 context encoder레이어의 아웃풋\r\n  $$PE(.)$$: positional encoding function\r\n\r\n### 4. Slot-Turn Attention\r\n\r\n<img src=\"slot-turn_attention.png\">\r\n\r\n- turn-level relevant information을 contextual representation에서 검출해내기 위해 사용\r\n- $$c_{s,t}^{turn} = MultiHead(h^s, c_{s,\\leq t}^{ctx},c_{s,\\leq t}^{ctx})$$\r\n- 이로인해 word-level and turn-level 의 relevant information을 historical dialogues에서 얻어낼 수 있다.\r\n\r\n### 5. Global-Local Fusion Gate\r\n\r\n<img src=\"global-local-fusion_gate.png\">\r\n\r\n- global context와 local utterance의 균형을 맞추기 위해, contextual information과 current turn information의 비율을 조절함.\r\n- $$c_{s,t}^{word},~~ c_{s,t}^{turn}$$에 따라 global과 local정보가 어떻게 결합되어야할지 알려주는 fusion gate mechanism을 사용\r\n- $$g_{s,t} = \\sigma(W_g\\bigodot[c_{s,t}^{word};c_{s,t}^{turn}])$$\r\n- $$c_{s,t}^gate = g_{s,t}\\bigotimes c_{s,t}^{word} + (1-g_{s,t}\\bigotimes c_{s,t}^{turn})$$\r\n  - $$W_g \\in R^{2d\\times d}$$\r\n  - $$\\sigma$$: Sigmoid\r\n  - $$\\bigodot$$, $$\\bigotimes$$\r\n- $$o_{s,t}$$ = LayerNorm(Linear(Dropout($c_{s,t}^{gate}$)))\r\n- **value $$v_t$$에 대한 확률분포와 training objective**  \r\n  $$p(v_t|U_{\\leq t},~R_{\\leq t}, s) = exp(-||o_{s,t} - h_t^v||2)\\over {\\sum{v'\\in V_s}exp(-||o_{s,t}-h_t^{v'}||2)}$$  \r\n  *$$L{dst}$$* $$= \\sum_{s\\in S}\\sum ^T_{t = 1}-log(p(\\hat v_t|U_{\\leq t},~R_{\\leq t}, s))$$  \r\n  - $$V_s$$: candidate value set of slot s\r\n  - $$\\hat v_t \\in V_s$$: ground-truth value of slot s\r\n\r\n---\r\n\r\n## State Transition Prediction\r\n\r\n<img src=\"state_transition_prediction.png\">\r\n\r\n- relevant context를 더 잘 포착하기 위해, auxiliary binary classification task사용.\r\n- $$c_{s,t}^{stp} = tanh(W_c \\odot c_{s,t}^{gate})$$\r\n- $$p_{s,t}^{stp} = \\sigma (W_p \\odot [c_{s,t}^{stp};c_{s, t-1}^{stp}])$$\r\n  - $$W_c \\in \\R^{d\\times d}$$\r\n  - $$W_c \\in \\R^{2d}$$\r\n  - $$t = 1$$일때는 $$c_{s,t}^{stp}$$와 zero vectors를 concat함.\r\n- binary CE loss ($$y_{s,t}^{stp}$$: ground-truth transition labels // $$p_{s,t}^{stp}$$: transition probability)\r\n- $$L_{stp} = \\sum_{s\\in S}\\sum_{t = 1}^T -y_{s,t}^{stp}~.~log(p_{s,t}^{stp})$$\r\n\r\n---\r\n\r\n## Adaptive Objective\r\n\r\n- hard slots와 samples에 관한 optimization을 encourage한다.\r\n- all slots의 learning을 balancing함.\r\n- $$acc_s^{val}$$: accuracy of slot s on validation set\r\n- `slot-level difficulty`\r\nif $$acc_s^{val} \\leq acc_{s'}^{val}$$;  \r\n  → slot s 가 slot s'보다 더 어려운 것.  \r\n  → $$\\alpha$$: slot-level difficulty  \r\n  - $$\\alpha_s = {1 - acc_s^{val} \\over {\\sum_{s'\\in S} 1-acc_{s'}^{val}}} \\cdot |S|$$\r\n- `sample-level difficulty`  \r\n  → Suppose there are two samples $$\\left\\{(U_t, R_t),(s, v_t)\\right\\}$$ and $$\\left\\{(U_{t'}, R_{t'}),(s', v_{t'})\\right\\}$$  \r\n  → 만약 former confidence 가 latter보다 더 낮다면, 첫번째 sample이 두번째보다 더 어려운 것.  \r\n  → $$\\beta$$: sample level difficulty  \r\n  - $$\\beta(s, v_t) = (1 - p(s, v_t))^\\gamma$$\r\n  - $$p(s,v_t)$$: confidence of sample $$(U_t, R_t),(s, v_t)$$  \r\n  $$\\gamma$$: hyper-parameter\r\n- $$L_{adapt}(s,v_t) = -\\alpha_s\\beta(s, v_t)log(p(s, v_t))$$\r\n- slot s가 평균 slot의 difficulty보다 높다면, $$\\alpha_s$$는 s에 대한 loss를 키울 것이다. 비슷하게, sample의 optimization이 low confidence를 갖고 있다면 loss는 커질것이다.\r\n\r\n---\r\n\r\n### Optimization\r\n\r\n- During joint training, we optimize the sum of these two loss functions as following  \r\n  $$L_{joint} = L_{dst} + L_{stp}$$  \r\n- At the fine-tuning phase, we adopt the adaptive objective to fine-tune DST task as following  \r\n  $$L_{finetune} = \\sum_{s\\in S}\\sum^T_{t=1}L_{adapt(s, \\hat v_t)}$$  \r\n\r\n---\r\n\r\n## 참고 자료\r\n\r\n- 논문: [A Contextual Hierarchical Attention Network with Adaptive Objective for Dialogue State Tracking](https://www.aclweb.org/anthology/2020.acl-main.563.pdf)\r\n- GitHub: [CHAN-DST](https://github.com/smartyfh/CHAN-DST)\r\n"},{"excerpt":"TRADE 1. 전체 프로세스 TRADE 대화를 인코더를 통해 인코딩한다. 인코딩된 대화와 슬롯으로 를 만들고, 이를 바탕으로 와 를 생성하여 로 슬롯에 해당하는 value를 찾는다. 대화와 슬롯으로 만들어진 를 사용하여 를 만들고, 를 통해 slot…","fields":{"slug":"/TRADE/"},"frontmatter":{"date":"April 30, 2021","title":"Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems","tags":["Paper Review","DST","Dialogue State Tracking","TRADE"]},"rawMarkdownBody":"\r\n## TRADE\r\n\r\n### 1. 전체 프로세스\r\n\r\n![TRADE](https://github.com/jasonwu0731/trade-dst/raw/master/plot/model.png)\r\n\r\n1. 대화를 인코더를 통해 인코딩한다.\r\n2. 인코딩된 대화와 슬롯으로 $$h$$를 만들고, 이를 바탕으로 $$p_{value}$$와 $$p_{hist}$$를 생성하여 $$p_{final}$$로 슬롯에 해당하는 value를 찾는다.\r\n3. 대화와 슬롯으로 만들어진 $$h_{j0}$$를 사용하여 $$p_{hist}$$를 만들고, $$c_{j0}$$를 통해 slot의 value를 사용할지 결정한다.\r\n\r\n---\r\n\r\n### 2. Definition (Terminology)\r\n\r\n- $$U_{T}$$: User Utterence\r\n- $$R_{T}$$: System Response\r\n- $$X_t = \\{ (U_{t-l}, R_{t-l}), ... (U_{t}, R_{t})\\}$$:  Utterance-Response pair\r\n- $$D = \\{ D_{1}, ..., D_{N} \\}$$: Domain\r\n- $$S = \\{ S_{1}, ..., S_{M} \\}$$: Slot\r\n- $$Y^{value}_{j}$$: Value\r\n- $$B = \\{ B_{1}, ..., B_{T} \\}$$: Tuple\r\n\r\n### 3. Utterance Encoder\r\n\r\n<img src=\"utterance_encoder.png\">\r\n\r\n- 논문에서는 **Bi-directional GRU**사용 (어떤 종류의 Encoder로도 대체 가능)\r\n- Input: $$X_t = \\{ (U_{t-l}, R_{t-l}), ... (U_{t}, R_{t})\\}$$\r\n  - 슬라이딩 윈도우처럼 $$l$$값에 따라 $$t$$번째 턴에는 $$t-l$$부터 $$t$$까지의 대화쌍을 살펴봄\r\n    - 베이스라인 코드에서는 $$t$$ 턴에는 처음부터 $$t$$ 턴까지 대화를 모두 봄\r\n  - 더 구체적으로 말하자면, dialougue history 의 **모든 단어**를 **concatenation** 한 것\r\n  - $$d_{emb}$$ 차원을 지님\r\n- $$t$$ 턴까지의 대화쌍에서 토큰들의 관계를 알 수 있다.\r\n\r\n### 4. State Generator\r\n\r\n<img src=\"state_generator.png\">\r\n\r\n- Bi-directional GRU decoder 사용\r\n- $$t$$ 턴까지의 대화 인코딩 $$h_t$$에 대해 max_length(value 중 가장 토큰을 많이 가진 길이)만큼 디코더로 디코딩 진행\r\n- Copy mechanism을 통해 input dialougue의 정보를 활용하여 slot value를 generate\r\n- 처음에는 도메인-슬롯의 임베딩 sum 을 입력으로 넣어주고 이를 통해 value 의 첫 토큰이 나온다.  \r\n  다음으로는 이 토큰을 입력으로 넣어주고.. 계속하여 알맞는 value 를 뽑아낸다 (서울 + 롯데 + 호텔).  \r\n  그러다 special token 이 나오면 value 생성을 그만한다.\r\n- Pointer-Generator 방법 사용\r\n  - 위에서 생성된 $$h^{dec}_{j0}$$로 $$p_{vocab}$$(대화와 슬롯을 봤을 때 vocab 에서는 어떤 단어와 유사한지), $$p_{hist}$$(대화와 슬롯을 봤을 때 이제까지의 대화 중에서는 어떤 단어와 유사한지)를 구한다.\r\n  - **Vocab의 분포**와 **dialogue history의 분포**를 하나의 분포로 결합\r\n\r\n<img src=\"P_vocab, P_history.png\">\r\n\r\n- $$P^{vocab}_{jk}$$는 Utterence Encoder에서 나타나는 (domain, slot) $$j$$번째, $$k$$번째 value 생성 차례의 vocab 확률 분포를 나타냄\r\n  - 여기서 $$E \\in \\R^{|V| \\times d_{hdd}}$$를 나타내고, Trainable Embedding (vocab 개수 x dimension)\r\n- $$P^{history}_{jk}$$는 Utterence Encoder에서 나타나는 (domain, slot) $$j$$번째, $$k$$번째 value 생성 차례의 history 확률 분포를 나타냄\r\n  - 여기서 $$H_{t}$$는 Encode된 dialouge history를 나타냄\r\n\r\n<img src=\"P_final.png\">\r\n\r\n- $$P^{final}_{jk}$$는 $$P^{vocab}_{jk}$$, $$P^{history}_{jk}$$의 $$P^{gen}_{jk}, 1-P^{gen}_{jk}$$만큼의 확률분포를 곱해서 생성\r\n\r\n<img src=\"p_gen.png\">\r\n\r\n- $$P^{gen}_{jk}$$는 다음 원소들로 구성됨\r\n  - $$W_{1}$$: 가중치\r\n  - $$h^{dec}_{jk}$$: (domain, slot) pair와 Utterance Encoding을 가지고 만들어진 hidden state vector\r\n  - $$w_{jk}$$: word embedding\r\n  - $$c_{jk}$$: context vector\r\n  - ;는 concat을 나타냄\r\n\r\n### 5. Slot Gate\r\n\r\n<img src=\"slot_gate.png\">\r\n\r\n- Slot Generator 에서 생성된 $$h^{dec}_{j0}$$(대화와 슬롯의 관계 정보)를 통해 $$p_{history}$$를 만들고, 이를 사용해 $$c_{j0}$$(Context vecto)를 만들 수 있다. Slot Gate는 Context vector로부터 slot의 존재 여부를 알아내는 역할을 수행\r\n  - PTR, DONTCARE, NONE 3가지 label로 출력\r\n  - PTR이 나오면 value를 Generate\r\n  - DONTCARE, NONE이 나오면 Ignore\r\n\r\n<img src=\"G_j.png\">\r\n\r\nSlot gate $$G_{j}$$는 다음 원소로 구성됨\r\n\r\n- $$W_{g}$$: 가중치\r\n- $$c_{j0}$$: Context vector(first decoder Hidden state)\r\n\r\n### 6. Optimization\r\n\r\n<img src=\"optimization.png\">\r\n\r\n> 출처 : [https://www.youtube.com/watch?v=nuclwoebdEM](https://www.youtube.com/watch?v=nuclwoebdEM)\r\n\r\n- 최종 Loss는 다음으로 구성됨\r\n\r\n$$L = \\alpha L_{g} + \\beta L_{v}$$\r\n\r\n# 결과\r\n\r\n### 1. Few/Zero-shot 실험 셋팅\r\n\r\n- Target Domain을 학습 데이터에서 제외\r\n    - 나머지 Source Domain의 데이터로 학습 후, Target Domain에 대한 성능 측정\r\n- Few-Shot의 경우 Target Domain에 대한 아래 방법들로 1%의 Training data만 사용하여 학습\r\n    - Elastic Weight Consolidation (EWC)\r\n    - Gradient Episodic Memory (GEM)\r\n    - Naive Fine-tuning (Naive)\r\n\r\n<img src=\"few, zero.png\">\r\n\r\n### 2. 실험 결과\r\n\r\n<img src=\"result.png\">\r\n\r\n- **첫번째는 하나의 Domain만 제외(ex. Except Hotel)해서 훈련시킴**\r\n    - 이후, 제외시킨 Domain (ex. Hotel)의 1% data를 fine-tuning했을 때의 결과\r\n    - GEM을 사용했을 때 가장 좋은 성능이 나오는 것을 알 수 있음\r\n\r\n- **두번째는 하나의 Domain만 훈련시킨 후 (ex. Hotel) 측정하는 방법**\r\n    - Train, Taxi Domain에서 높은값들이 나오는 이유는 Domain끼리 겹치는 slot이 존재하기 때문\r\n\r\n# 평가\r\n\r\n- 새로운 domain(unseen domain)에 대해 robust한 모델\r\n- Pointer-Generator를 사용해 Open-Vocabulary 방법을 사용\r\n- turn마다 모든 슬롯 $$j$$에 대해 value를 생성하는 점이 비효율적임\r\n\r\n---\r\n\r\n## 참고 자료\r\n\r\n- 논문: [Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems](https://arxiv.org/abs/1905.08743)\r\n- 영상: [[Paper Review] Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems - KoreaUniv DSBA](https://www.youtube.com/watch?v=nuclwoebdEM)\r\n- GitHub: [TRADE](https://github.com/jasonwu0731/trade-dst)\r\n"}]}},"pageContext":{}},"staticQueryHashes":[]}