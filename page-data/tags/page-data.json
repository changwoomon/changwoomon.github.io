{"componentChunkName":"component---src-pages-tags-jsx","path":"/tags/","result":{"data":{"site":{"siteMetadata":{"title":"changwoomon's blog"}},"allMarkdownRemark":{"group":[{"fieldValue":"CHAN-DST","totalCount":1},{"fieldValue":"DST","totalCount":4},{"fieldValue":"Dialogue State Tracking","totalCount":5},{"fieldValue":"EEG","totalCount":1},{"fieldValue":"Paper Review","totalCount":6},{"fieldValue":"SOM-DST","totalCount":2},{"fieldValue":"TRADE","totalCount":1},{"fieldValue":"TabNet","totalCount":1},{"fieldValue":"Tabular Data","totalCount":1},{"fieldValue":"Transformer-DST","totalCount":1}],"nodes":[{"excerpt":"Proposed CNN architecture stratified tenfold cross-validation Adam optimization with a learning rate of 0.0001 activation functions such as Relu for all layers softmax for the last layer dropout is…","fields":{"slug":"/Parkinson_1/"},"frontmatter":{"date":"September 07, 2021","update":"Sep 07, 2021","title":"A deep learning approach for Parkinson's disease diagnosis from EEG signals","tags":["Paper Review","EEG","Dialogue State Tracking","SOM-DST"]}},{"excerpt":"TabNet 1. Abstract Sequential attention을 사용해서 각각의 decision step마다 여러 feature들 중에 중요한 feature들만 고름 tabular data의 self-supervised learning도 가능함 (label이 안되어있는..) 2. Related work 1. Feature selection…","fields":{"slug":"/TabNet/"},"frontmatter":{"date":"June 08, 2021","update":"Jun 08, 2021","title":"TabNet: Attentive Interpretable Tabular Learning","tags":["Paper Review","Tabular Data","TabNet"]}},{"excerpt":"Transformer-DST 1. Key Idea 이전 논문들에서는 Encoder에서 BERT를 사용하지만, Value Generation부분에서는 RNN Decoder를 사용하는 아이러니.. Purely Transformer-based framework를 사용\n즉, Single BERT가 Encoder와 Decoder 모두에서 work…","fields":{"slug":"/Transformer-DST/"},"frontmatter":{"date":"May 12, 2021","update":"May 12, 2021","title":"Jointly Optimizing State Operation Prediction and Value Generation for Dialogue State Tracking","tags":["Paper Review","DST","Dialogue State Tracking","Transformer-DST"]}},{"excerpt":"SOM-DST SOM-DST 기존 모델의 문제점 Ontology-based DST 실제 시나리오에 잘 대응하지 못함 unseen value를 처리할 수 없음 ontology가 많으면 처리 시간이 오래 걸림 Open-vocab-based DST (TRADE) turn마다 slot의 모든 value를 생성해야해서 비효율적임 Definition : turn…","fields":{"slug":"/SOM-DST/"},"frontmatter":{"date":"May 10, 2021","update":"May 10, 2021","title":"Efficient Dialogue State Tracking by Selectively Overwriting Memory","tags":["Paper Review","DST","Dialogue State Tracking","SOM-DST"]}},{"excerpt":"CHAN-DST slot imbalance 문제를 해결하고자 adaptive objective를 도입 a contextual hierarchical attention network (CHAN)를 사용: dislogue history에서 relevant context를 찾기 위함 → 각 턴의 발화로부터 word-level…","fields":{"slug":"/CHAN-DST/"},"frontmatter":{"date":"May 07, 2021","update":"May 07, 2021","title":"A Contextual Hierarchical Attention Network with Adaptive Objective for Dialogue State Tracking","tags":["Paper Review","DST","Dialogue State Tracking","CHAN-DST"]}},{"excerpt":"TRADE 1. 전체 프로세스 TRADE 대화를 인코더를 통해 인코딩한다. 인코딩된 대화와 슬롯으로 를 만들고, 이를 바탕으로 와 를 생성하여 로 슬롯에 해당하는 value를 찾는다. 대화와 슬롯으로 만들어진 를 사용하여 를 만들고, 를 통해 slot의 value를 사용할지 결정한다. 2. Definition (Terminology) : User…","fields":{"slug":"/TRADE/"},"frontmatter":{"date":"April 30, 2021","update":"Apr 30, 2021","title":"Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems","tags":["Paper Review","DST","Dialogue State Tracking","TRADE"]}}]}},"pageContext":{}},"staticQueryHashes":[]}